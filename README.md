# Project Road Segmentation

For this task, we are provided a set of satellite images acquired 
from GoogleMaps. We are also provided ground-truth images where each pixel is labeled 
as road or background. 

Our task is to train a classifier to segment roads in these images, i.e. 
assigns a label `road=1, background=0` to each pixel.

Submission system environment setup:

1. The original dataset is available from the 
[CrowdAI page](https://www.crowdai.org/challenges/epfl-ml-road-segmentation).
2. The submission is also performed on the [CrowdAI page](https://www.crowdai.org/challenges/epfl-ml-road-segmentation).


Evaluation Metric:
 [F1 score](https://en.wikipedia.org/wiki/F1_score)

## Libraries

Contained in the `requirements.txt` file, to install do:

        pip install requirements.txt

The main libraries used are tensorflow/keras for the models and numpy for image transformations. We also use additional
libraries such as imageio to create our prediction images and matplotlib to visualise some results.

## Project Structure

* `models`: contains trained models h5 files
* `code` : contains the project code
* `data`: contains the project data
* `predictions`: contains the predictions made by our models

Note: The `data` folder has to be downloaded (see instructions below), 
`predictions` is automatically generated while performing predictions

### Inside `code`

* `models`: Folder with all models, namely CNNs, Dilated Residual Networks, Unet and TransUnet.
* `data_augmentation.py`: File with functions related to data augmentation techniques
* `dataset_loader.py`: Implemented a dataset loader, in order to remove RAM memory bootleneck
* `helper_functions.py`: F1 score and other utils
* `model_runner.py`: Centralized class for different model running use cases
* `post_processing.py`: Functions that apply post processing to predictions
* `pre_processing.py`: Functions for data preprocessing 
* `run.py`: Executable file with multiple desired use cases, such as training, predicting and tuning
* `visualize_results.py`: Functions responsible for patch to image conversion
* `mask_to_submission.py`: Convert image to submission folder utils

### Inside `data`

* `test_set_images` contains the test images provided for the competition on AICrowd, for which we do not have ground 
truths
* `training` contains the training data. It contains both the original images from the dataset and images generated by
us through image transformation, for data augmentation purposes. The subfolder `images` contains the satellite images
and `groundtruth` contains the corresponding ground truth segmentations.

### Inside `models`

The `models` folder contains one folder per model created, each of which contains the h5 file of the said model 
that we have trained.

### Inside `prediction`

Once generated, the predictions of each model can be found in this folder inside the subfolder with the model's name.

## Run the code

#### Setup

* Clone this repository
* Download the data from [this link](https://drive.google.com/drive/folders/1_YtQPBBmE1QRZ4PQ2T8X8ysrAJNlAs7n?usp=sharing),
unzip it and place the data folder at the root level of this repository
* export the PYTHONPATH to equal the project path

        export PYTHONPATH=<project_path>


#### Running

Go to the `code`

In order to perform model training, run in the terminal

    python3 run.py train <model_name>

In order to perform model predictions, run in the terminal

    python3 run.py predict <model_name> <model_version>

In order to perform model tuning, run in the terminal (warning: takes really long!)

    python3 run.py tune <model_name> <k_folds>
    
In order to reproduce our best results, run in the terminal

    python3 run.py predict ensemble 0

*Note: Your results might differ a little from ours, since we were not able to properly seed the training. We tried
seeding numpy and tensorflow, but the parallelization taking place at GPU level makes it impossible to have fully
reproducible results. See [here](https://machinelearningmastery.com/reproducible-results-neural-networks-keras/) for
further details.*
